Start:20220123_21_00_09

Seed Set: True , Seed :3
Namespace(batch_size=64, img_size=256, logdir='./log/20220123_21_00_09.log', lr=0.001, model='resnet18', n_epochs=200, optim='SGD', pretrained=True, resume=False, root='./data')

Using cuda device

ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=4, bias=True)
)
Train_loss: 1.688645  Train_acc: 0.231250  [1/200]
Train_loss: 1.632276  Train_acc: 0.242188  [2/200]
Train_loss: 1.583380  Train_acc: 0.254167  [3/200]
Train_loss: 1.534110  Train_acc: 0.273438  [4/200]
Train_loss: 1.494608  Train_acc: 0.283125  [5/200]
Train_loss: 1.429221  Train_acc: 0.307292  [6/200]
Train_loss: 1.368164  Train_acc: 0.333929  [7/200]
Train_loss: 1.307720  Train_acc: 0.362109  [8/200]
Train_loss: 1.238060  Train_acc: 0.398611  [9/200]
Train_loss: 1.149487  Train_acc: 0.431563  [10/200]
Train_loss: 1.084762  Train_acc: 0.460227  [11/200]
Train_loss: 1.014683  Train_acc: 0.490365  [12/200]
Train_loss: 0.910530  Train_acc: 0.519471  [13/200]
Train_loss: 0.870918  Train_acc: 0.545982  [14/200]
Train_loss: 0.768637  Train_acc: 0.570625  [15/200]
Train_loss: 0.686210  Train_acc: 0.593555  [16/200]
Train_loss: 0.635456  Train_acc: 0.613971  [17/200]
Train_loss: 0.557940  Train_acc: 0.633681  [18/200]
Train_loss: 0.495794  Train_acc: 0.651480  [19/200]
Train_loss: 0.450782  Train_acc: 0.668750  [20/200]
Train_loss: 0.398860  Train_acc: 0.684375  [21/200]
Train_loss: 0.376721  Train_acc: 0.698580  [22/200]
Train_loss: 0.341131  Train_acc: 0.711685  [23/200]
Train_loss: 0.331251  Train_acc: 0.723698  [24/200]
Train_loss: 0.305153  Train_acc: 0.734625  [25/200]
Train_loss: 0.281013  Train_acc: 0.744832  [26/200]
Train_loss: 0.278497  Train_acc: 0.754282  [27/200]
Train_loss: 0.253211  Train_acc: 0.763058  [28/200]
Train_loss: 0.255942  Train_acc: 0.771228  [29/200]
Train_loss: 0.245786  Train_acc: 0.778854  [30/200]
Train_loss: 0.233793  Train_acc: 0.785988  [31/200]
Train_loss: 0.228810  Train_acc: 0.792676  [32/200]
Train_loss: 0.224367  Train_acc: 0.798958  [33/200]
Train_loss: 0.216668  Train_acc: 0.804871  [34/200]
Train_loss: 0.217834  Train_acc: 0.810446  [35/200]
Train_loss: 0.206881  Train_acc: 0.815712  [36/200]
Train_loss: 0.204859  Train_acc: 0.820693  [37/200]
Train_loss: 0.205805  Train_acc: 0.825411  [38/200]
Train_loss: 0.204310  Train_acc: 0.829888  [39/200]
Train_loss: 0.204075  Train_acc: 0.834141  [40/200]
Train_loss: 0.198333  Train_acc: 0.838186  [41/200]
Train_loss: 0.194609  Train_acc: 0.842039  [42/200]
Train_loss: 0.195275  Train_acc: 0.845712  [43/200]
Train_loss: 0.192970  Train_acc: 0.849219  [44/200]
Train_loss: 0.195521  Train_acc: 0.852569  [45/200]
Train_loss: 0.195829  Train_acc: 0.855774  [46/200]
Train_loss: 0.192909  Train_acc: 0.858843  [47/200]
Train_loss: 0.192998  Train_acc: 0.861784  [48/200]
Train_loss: 0.194163  Train_acc: 0.864605  [49/200]
Train_loss: 0.192381  Train_acc: 0.867313  [50/200]
Train_loss: 0.187944  Train_acc: 0.869914  [51/200]
Train_loss: 0.188473  Train_acc: 0.872416  [52/200]
Train_loss: 0.187935  Train_acc: 0.874823  [53/200]
Train_loss: 0.186924  Train_acc: 0.877141  [54/200]
Train_loss: 0.183975  Train_acc: 0.879375  [55/200]
Train_loss: 0.184525  Train_acc: 0.881529  [56/200]
Train_loss: 0.187397  Train_acc: 0.883607  [57/200]
Train_loss: 0.181823  Train_acc: 0.885614  [58/200]
Train_loss: 0.181085  Train_acc: 0.887553  [59/200]
Train_loss: 0.187610  Train_acc: 0.889427  [60/200]
Train_loss: 0.183240  Train_acc: 0.891240  [61/200]
Train_loss: 0.180674  Train_acc: 0.892994  [62/200]
Train_loss: 0.181044  Train_acc: 0.894692  [63/200]
Train_loss: 0.181627  Train_acc: 0.896338  [64/200]
Train_loss: 0.181439  Train_acc: 0.897933  [65/200]
Train_loss: 0.182684  Train_acc: 0.899479  [66/200]
Train_loss: 0.183649  Train_acc: 0.900979  [67/200]
Train_loss: 0.179247  Train_acc: 0.902436  [68/200]
Train_loss: 0.181088  Train_acc: 0.903850  [69/200]
Train_loss: 0.180433  Train_acc: 0.905223  [70/200]
Train_loss: 0.181971  Train_acc: 0.906558  [71/200]
Train_loss: 0.177882  Train_acc: 0.907856  [72/200]
Train_loss: 0.180692  Train_acc: 0.909118  [73/200]
Train_loss: 0.181894  Train_acc: 0.910346  [74/200]
Train_loss: 0.177989  Train_acc: 0.911542  [75/200]
Train_loss: 0.178830  Train_acc: 0.912706  [76/200]
Train_loss: 0.177615  Train_acc: 0.913839  [77/200]
Train_loss: 0.176878  Train_acc: 0.914944  [78/200]
Train_loss: 0.176565  Train_acc: 0.916021  [79/200]
Train_loss: 0.178537  Train_acc: 0.917070  [80/200]
Train_loss: 0.177392  Train_acc: 0.918094  [81/200]
Train_loss: 0.176867  Train_acc: 0.919093  [82/200]
Train_loss: 0.176031  Train_acc: 0.920068  [83/200]
Train_loss: 0.176558  Train_acc: 0.921019  [84/200]
Train_loss: 0.179368  Train_acc: 0.921949  [85/200]
Train_loss: 0.178269  Train_acc: 0.922856  [86/200]
Train_loss: 0.177438  Train_acc: 0.923743  [87/200]
Train_loss: 0.177289  Train_acc: 0.924609  [88/200]
Train_loss: 0.174942  Train_acc: 0.925456  [89/200]
Train_loss: 0.176570  Train_acc: 0.926285  [90/200]
Train_loss: 0.176335  Train_acc: 0.927095  [91/200]
Train_loss: 0.176048  Train_acc: 0.927887  [92/200]
Train_loss: 0.175424  Train_acc: 0.928663  [93/200]
Train_loss: 0.174662  Train_acc: 0.929422  [94/200]
Train_loss: 0.175485  Train_acc: 0.930164  [95/200]
Train_loss: 0.175754  Train_acc: 0.930892  [96/200]
Train_loss: 0.175690  Train_acc: 0.931604  [97/200]
Train_loss: 0.175506  Train_acc: 0.932302  [98/200]
Train_loss: 0.176154  Train_acc: 0.932986  [99/200]
Train_loss: 0.176307  Train_acc: 0.933656  [100/200]
Train_loss: 0.175316  Train_acc: 0.934313  [101/200]
Train_loss: 0.174018  Train_acc: 0.934957  [102/200]
Train_loss: 0.175777  Train_acc: 0.935589  [103/200]
Train_loss: 0.174605  Train_acc: 0.936208  [104/200]
Train_loss: 0.174354  Train_acc: 0.936815  [105/200]
Train_loss: 0.174989  Train_acc: 0.937412  [106/200]
Train_loss: 0.174681  Train_acc: 0.937996  [107/200]
Train_loss: 0.174620  Train_acc: 0.938571  [108/200]
Train_loss: 0.175581  Train_acc: 0.939134  [109/200]
Train_loss: 0.176212  Train_acc: 0.939688  [110/200]
Train_loss: 0.177651  Train_acc: 0.940231  [111/200]
Train_loss: 0.173876  Train_acc: 0.940765  [112/200]
Train_loss: 0.174696  Train_acc: 0.941289  [113/200]
Train_loss: 0.173751  Train_acc: 0.941804  [114/200]
Train_loss: 0.174499  Train_acc: 0.942310  [115/200]
Train_loss: 0.174184  Train_acc: 0.942807  [116/200]
Train_loss: 0.173686  Train_acc: 0.943296  [117/200]
Train_loss: 0.173940  Train_acc: 0.943776  [118/200]
Train_loss: 0.173731  Train_acc: 0.944249  [119/200]
Train_loss: 0.178619  Train_acc: 0.944714  [120/200]
Train_loss: 0.174723  Train_acc: 0.945170  [121/200]
Train_loss: 0.173305  Train_acc: 0.945620  [122/200]
Train_loss: 0.174396  Train_acc: 0.946062  [123/200]
Train_loss: 0.173292  Train_acc: 0.946497  [124/200]
Train_loss: 0.173059  Train_acc: 0.946925  [125/200]
Train_loss: 0.176081  Train_acc: 0.947346  [126/200]
Train_loss: 0.173500  Train_acc: 0.947761  [127/200]
Train_loss: 0.173120  Train_acc: 0.948169  [128/200]
Train_loss: 0.174230  Train_acc: 0.948571  [129/200]
Train_loss: 0.174142  Train_acc: 0.948966  [130/200]
Train_loss: 0.172911  Train_acc: 0.949356  [131/200]
Train_loss: 0.173586  Train_acc: 0.949740  [132/200]
Train_loss: 0.173262  Train_acc: 0.950117  [133/200]
Train_loss: 0.175350  Train_acc: 0.950490  [134/200]
Train_loss: 0.173081  Train_acc: 0.950856  [135/200]
Train_loss: 0.173352  Train_acc: 0.951218  [136/200]
Train_loss: 0.174138  Train_acc: 0.951574  [137/200]
Train_loss: 0.174318  Train_acc: 0.951925  [138/200]
Train_loss: 0.174720  Train_acc: 0.952271  [139/200]
Train_loss: 0.172619  Train_acc: 0.952612  [140/200]
Train_loss: 0.173031  Train_acc: 0.952948  [141/200]
Train_loss: 0.173661  Train_acc: 0.953279  [142/200]
Train_loss: 0.172226  Train_acc: 0.953606  [143/200]
Train_loss: 0.174814  Train_acc: 0.953928  [144/200]
Train_loss: 0.173177  Train_acc: 0.954246  [145/200]
Train_loss: 0.173148  Train_acc: 0.954559  [146/200]
Train_loss: 0.172243  Train_acc: 0.954868  [147/200]
Train_loss: 0.173078  Train_acc: 0.955173  [148/200]
Train_loss: 0.173416  Train_acc: 0.955474  [149/200]
Train_loss: 0.173594  Train_acc: 0.955771  [150/200]
Train_loss: 0.173681  Train_acc: 0.956064  [151/200]
Train_loss: 0.173066  Train_acc: 0.956353  [152/200]
Train_loss: 0.173719  Train_acc: 0.956638  [153/200]
Train_loss: 0.172799  Train_acc: 0.956920  [154/200]
Train_loss: 0.173210  Train_acc: 0.957198  [155/200]
Train_loss: 0.172651  Train_acc: 0.957472  [156/200]
Train_loss: 0.173723  Train_acc: 0.957743  [157/200]
Train_loss: 0.173333  Train_acc: 0.958010  [158/200]
Train_loss: 0.172586  Train_acc: 0.958274  [159/200]
Train_loss: 0.173880  Train_acc: 0.958535  [160/200]
Train_loss: 0.173554  Train_acc: 0.958793  [161/200]
Train_loss: 0.172734  Train_acc: 0.959047  [162/200]
Train_loss: 0.172641  Train_acc: 0.959298  [163/200]
Train_loss: 0.173340  Train_acc: 0.959546  [164/200]
Train_loss: 0.174085  Train_acc: 0.959792  [165/200]
Train_loss: 0.172565  Train_acc: 0.960034  [166/200]
Train_loss: 0.172651  Train_acc: 0.960273  [167/200]
Train_loss: 0.173969  Train_acc: 0.960510  [168/200]
Train_loss: 0.173912  Train_acc: 0.960743  [169/200]
Train_loss: 0.172438  Train_acc: 0.960974  [170/200]
Train_loss: 0.174218  Train_acc: 0.961202  [171/200]
Train_loss: 0.174799  Train_acc: 0.961428  [172/200]
Train_loss: 0.172838  Train_acc: 0.961651  [173/200]
Train_loss: 0.172606  Train_acc: 0.961871  [174/200]
Train_loss: 0.173267  Train_acc: 0.962089  [175/200]
Train_loss: 0.173230  Train_acc: 0.962305  [176/200]
Train_loss: 0.174314  Train_acc: 0.962518  [177/200]
Train_loss: 0.173107  Train_acc: 0.962728  [178/200]
Train_loss: 0.173419  Train_acc: 0.962936  [179/200]
Train_loss: 0.172241  Train_acc: 0.963142  [180/200]
Train_loss: 0.173339  Train_acc: 0.963346  [181/200]
Train_loss: 0.172887  Train_acc: 0.963547  [182/200]
Train_loss: 0.173120  Train_acc: 0.963747  [183/200]
Train_loss: 0.174158  Train_acc: 0.963944  [184/200]
Train_loss: 0.173371  Train_acc: 0.964139  [185/200]
Train_loss: 0.173039  Train_acc: 0.964331  [186/200]
Train_loss: 0.173337  Train_acc: 0.964522  [187/200]
Train_loss: 0.172931  Train_acc: 0.964711  [188/200]
Train_loss: 0.172893  Train_acc: 0.964897  [189/200]
Train_loss: 0.172553  Train_acc: 0.965082  [190/200]
Train_loss: 0.172748  Train_acc: 0.965265  [191/200]
Train_loss: 0.173040  Train_acc: 0.965446  [192/200]
Train_loss: 0.174365  Train_acc: 0.965625  [193/200]
Train_loss: 0.173520  Train_acc: 0.965802  [194/200]
Train_loss: 0.172837  Train_acc: 0.965978  [195/200]
Train_loss: 0.172452  Train_acc: 0.966151  [196/200]
Train_loss: 0.173023  Train_acc: 0.966323  [197/200]
Train_loss: 0.172908  Train_acc: 0.966493  [198/200]
Train_loss: 0.172299  Train_acc: 0.966661  [199/200]
Train_loss: 0.173051  Train_acc: 0.966828  [200/200]
Test_acc: 0.650000

End:20220123_21_10_01
Implementation time:9m 51s

