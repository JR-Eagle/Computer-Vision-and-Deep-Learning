Start:20220123_23_31_08

Seed Set: True , Seed :3
Namespace(batch_size=64, img_size=256, logdir='./log/20220123_23_31_08.log', lr=0.001, model='resnet18_ft', n_epochs=200, optim='SGD', pretrained=True, resume=False, root='./data')

Using cuda device

ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=4, bias=True)
)
Train_loss: 1.679864  Train_acc: 0.234375  [1/200]
Train_loss: 1.639003  Train_acc: 0.223438  [2/200]
Train_loss: 1.598537  Train_acc: 0.226042  [3/200]
Train_loss: 1.570188  Train_acc: 0.229687  [4/200]
Train_loss: 1.542743  Train_acc: 0.246250  [5/200]
Train_loss: 1.512655  Train_acc: 0.261458  [6/200]
Train_loss: 1.498671  Train_acc: 0.276339  [7/200]
Train_loss: 1.458517  Train_acc: 0.300781  [8/200]
Train_loss: 1.427305  Train_acc: 0.322222  [9/200]
Train_loss: 1.402483  Train_acc: 0.339062  [10/200]
Train_loss: 1.373204  Train_acc: 0.355398  [11/200]
Train_loss: 1.354818  Train_acc: 0.372396  [12/200]
Train_loss: 1.295629  Train_acc: 0.388942  [13/200]
Train_loss: 1.248249  Train_acc: 0.408036  [14/200]
Train_loss: 1.244959  Train_acc: 0.423125  [15/200]
Train_loss: 1.170370  Train_acc: 0.440625  [16/200]
Train_loss: 1.136471  Train_acc: 0.456985  [17/200]
Train_loss: 1.092338  Train_acc: 0.473438  [18/200]
Train_loss: 1.057110  Train_acc: 0.488158  [19/200]
Train_loss: 1.001778  Train_acc: 0.504844  [20/200]
Train_loss: 0.920657  Train_acc: 0.519494  [21/200]
Train_loss: 0.868399  Train_acc: 0.535937  [22/200]
Train_loss: 0.780015  Train_acc: 0.552717  [23/200]
Train_loss: 0.730384  Train_acc: 0.568099  [24/200]
Train_loss: 0.714879  Train_acc: 0.581875  [25/200]
Train_loss: 0.651314  Train_acc: 0.594952  [26/200]
Train_loss: 0.643509  Train_acc: 0.607523  [27/200]
Train_loss: 0.617858  Train_acc: 0.618862  [28/200]
Train_loss: 0.552288  Train_acc: 0.630496  [29/200]
Train_loss: 0.492995  Train_acc: 0.642083  [30/200]
Train_loss: 0.530165  Train_acc: 0.652117  [31/200]
Train_loss: 0.495449  Train_acc: 0.661719  [32/200]
Train_loss: 0.470769  Train_acc: 0.670928  [33/200]
Train_loss: 0.411476  Train_acc: 0.680239  [34/200]
Train_loss: 0.416822  Train_acc: 0.689018  [35/200]
Train_loss: 0.364140  Train_acc: 0.697569  [36/200]
Train_loss: 0.366427  Train_acc: 0.705490  [37/200]
Train_loss: 0.358334  Train_acc: 0.713158  [38/200]
Train_loss: 0.355243  Train_acc: 0.720192  [39/200]
Train_loss: 0.317058  Train_acc: 0.727109  [40/200]
Train_loss: 0.378015  Train_acc: 0.733003  [41/200]
Train_loss: 0.309251  Train_acc: 0.739211  [42/200]
Train_loss: 0.310509  Train_acc: 0.745276  [43/200]
Train_loss: 0.290450  Train_acc: 0.751065  [44/200]
Train_loss: 0.282602  Train_acc: 0.756597  [45/200]
Train_loss: 0.282240  Train_acc: 0.761753  [46/200]
Train_loss: 0.270627  Train_acc: 0.766822  [47/200]
Train_loss: 0.262180  Train_acc: 0.771680  [48/200]
Train_loss: 0.267977  Train_acc: 0.776339  [49/200]
Train_loss: 0.249701  Train_acc: 0.780813  [50/200]
Train_loss: 0.259958  Train_acc: 0.785049  [51/200]
Train_loss: 0.247636  Train_acc: 0.789183  [52/200]
Train_loss: 0.234613  Train_acc: 0.793160  [53/200]
Train_loss: 0.235119  Train_acc: 0.796991  [54/200]
Train_loss: 0.225606  Train_acc: 0.800682  [55/200]
Train_loss: 0.230459  Train_acc: 0.804241  [56/200]
Train_loss: 0.228307  Train_acc: 0.807675  [57/200]
Train_loss: 0.221770  Train_acc: 0.810991  [58/200]
Train_loss: 0.233967  Train_acc: 0.814195  [59/200]
Train_loss: 0.221545  Train_acc: 0.817292  [60/200]
Train_loss: 0.243238  Train_acc: 0.820236  [61/200]
Train_loss: 0.216336  Train_acc: 0.823135  [62/200]
Train_loss: 0.216485  Train_acc: 0.825942  [63/200]
Train_loss: 0.225519  Train_acc: 0.828613  [64/200]
Train_loss: 0.208343  Train_acc: 0.831250  [65/200]
Train_loss: 0.206089  Train_acc: 0.833807  [66/200]
Train_loss: 0.204051  Train_acc: 0.836287  [67/200]
Train_loss: 0.201724  Train_acc: 0.838695  [68/200]
Train_loss: 0.208881  Train_acc: 0.841033  [69/200]
Train_loss: 0.205796  Train_acc: 0.843304  [70/200]
Train_loss: 0.205478  Train_acc: 0.845511  [71/200]
Train_loss: 0.199685  Train_acc: 0.847656  [72/200]
Train_loss: 0.204504  Train_acc: 0.849743  [73/200]
Train_loss: 0.199079  Train_acc: 0.851774  [74/200]
Train_loss: 0.213347  Train_acc: 0.853750  [75/200]
Train_loss: 0.209979  Train_acc: 0.855633  [76/200]
Train_loss: 0.196619  Train_acc: 0.857508  [77/200]
Train_loss: 0.200079  Train_acc: 0.859335  [78/200]
Train_loss: 0.194979  Train_acc: 0.861116  [79/200]
Train_loss: 0.194234  Train_acc: 0.862852  [80/200]
Train_loss: 0.195138  Train_acc: 0.864545  [81/200]
Train_loss: 0.193057  Train_acc: 0.866197  [82/200]
Train_loss: 0.195414  Train_acc: 0.867809  [83/200]
Train_loss: 0.191787  Train_acc: 0.869382  [84/200]
Train_loss: 0.193510  Train_acc: 0.870919  [85/200]
Train_loss: 0.188972  Train_acc: 0.872420  [86/200]
Train_loss: 0.191263  Train_acc: 0.873886  [87/200]
Train_loss: 0.195952  Train_acc: 0.875320  [88/200]
Train_loss: 0.187387  Train_acc: 0.876721  [89/200]
Train_loss: 0.189352  Train_acc: 0.878090  [90/200]
Train_loss: 0.188485  Train_acc: 0.879430  [91/200]
Train_loss: 0.185726  Train_acc: 0.880740  [92/200]
Train_loss: 0.185964  Train_acc: 0.882023  [93/200]
Train_loss: 0.186378  Train_acc: 0.883278  [94/200]
Train_loss: 0.186951  Train_acc: 0.884507  [95/200]
Train_loss: 0.186328  Train_acc: 0.885710  [96/200]
Train_loss: 0.190943  Train_acc: 0.886888  [97/200]
Train_loss: 0.195343  Train_acc: 0.888042  [98/200]
Train_loss: 0.187486  Train_acc: 0.889173  [99/200]
Train_loss: 0.186883  Train_acc: 0.890281  [100/200]
Train_loss: 0.192533  Train_acc: 0.891368  [101/200]
Train_loss: 0.184684  Train_acc: 0.892433  [102/200]
Train_loss: 0.183298  Train_acc: 0.893477  [103/200]
Train_loss: 0.185023  Train_acc: 0.894501  [104/200]
Train_loss: 0.184722  Train_acc: 0.895506  [105/200]
Train_loss: 0.187387  Train_acc: 0.896492  [106/200]
Train_loss: 0.185896  Train_acc: 0.897459  [107/200]
Train_loss: 0.192103  Train_acc: 0.898409  [108/200]
Train_loss: 0.183188  Train_acc: 0.899341  [109/200]
Train_loss: 0.182660  Train_acc: 0.900256  [110/200]
Train_loss: 0.183501  Train_acc: 0.901154  [111/200]
Train_loss: 0.190151  Train_acc: 0.902037  [112/200]
Train_loss: 0.185594  Train_acc: 0.902904  [113/200]
Train_loss: 0.182537  Train_acc: 0.903755  [114/200]
Train_loss: 0.184575  Train_acc: 0.904592  [115/200]
Train_loss: 0.182536  Train_acc: 0.905415  [116/200]
Train_loss: 0.184332  Train_acc: 0.906223  [117/200]
Train_loss: 0.182304  Train_acc: 0.907018  [118/200]
Train_loss: 0.183488  Train_acc: 0.907799  [119/200]
Train_loss: 0.180796  Train_acc: 0.908568  [120/200]
Train_loss: 0.182521  Train_acc: 0.909323  [121/200]
Train_loss: 0.187973  Train_acc: 0.910067  [122/200]
Train_loss: 0.183024  Train_acc: 0.910798  [123/200]
Train_loss: 0.181605  Train_acc: 0.911517  [124/200]
Train_loss: 0.182864  Train_acc: 0.912225  [125/200]
Train_loss: 0.180026  Train_acc: 0.912922  [126/200]
Train_loss: 0.179992  Train_acc: 0.913607  [127/200]
Train_loss: 0.183995  Train_acc: 0.914282  [128/200]
Train_loss: 0.182599  Train_acc: 0.914947  [129/200]
Train_loss: 0.183778  Train_acc: 0.915601  [130/200]
Train_loss: 0.181009  Train_acc: 0.916245  [131/200]
Train_loss: 0.181892  Train_acc: 0.916880  [132/200]
Train_loss: 0.185441  Train_acc: 0.917505  [133/200]
Train_loss: 0.179188  Train_acc: 0.918120  [134/200]
Train_loss: 0.184503  Train_acc: 0.918727  [135/200]
Train_loss: 0.183952  Train_acc: 0.919324  [136/200]
Train_loss: 0.185119  Train_acc: 0.919913  [137/200]
Train_loss: 0.192494  Train_acc: 0.920471  [138/200]
Train_loss: 0.179690  Train_acc: 0.921043  [139/200]
Train_loss: 0.181262  Train_acc: 0.921607  [140/200]
Train_loss: 0.181536  Train_acc: 0.922163  [141/200]
Train_loss: 0.190516  Train_acc: 0.922711  [142/200]
Train_loss: 0.183546  Train_acc: 0.923252  [143/200]
Train_loss: 0.180335  Train_acc: 0.923785  [144/200]
Train_loss: 0.181700  Train_acc: 0.924310  [145/200]
Train_loss: 0.178004  Train_acc: 0.924829  [146/200]
Train_loss: 0.188098  Train_acc: 0.925340  [147/200]
Train_loss: 0.186928  Train_acc: 0.925845  [148/200]
Train_loss: 0.180299  Train_acc: 0.926342  [149/200]
Train_loss: 0.184103  Train_acc: 0.926833  [150/200]
Train_loss: 0.180098  Train_acc: 0.927318  [151/200]
Train_loss: 0.180924  Train_acc: 0.927796  [152/200]
Train_loss: 0.183415  Train_acc: 0.928268  [153/200]
Train_loss: 0.183632  Train_acc: 0.928734  [154/200]
Train_loss: 0.183447  Train_acc: 0.929194  [155/200]
Train_loss: 0.180269  Train_acc: 0.929647  [156/200]
Train_loss: 0.178673  Train_acc: 0.930096  [157/200]
Train_loss: 0.182656  Train_acc: 0.930538  [158/200]
Train_loss: 0.179471  Train_acc: 0.930975  [159/200]
Train_loss: 0.178663  Train_acc: 0.931406  [160/200]
Train_loss: 0.180376  Train_acc: 0.931832  [161/200]
Train_loss: 0.180297  Train_acc: 0.932253  [162/200]
Train_loss: 0.179727  Train_acc: 0.932669  [163/200]
Train_loss: 0.178946  Train_acc: 0.933079  [164/200]
Train_loss: 0.178382  Train_acc: 0.933485  [165/200]
Train_loss: 0.178797  Train_acc: 0.933886  [166/200]
Train_loss: 0.178306  Train_acc: 0.934281  [167/200]
Train_loss: 0.180272  Train_acc: 0.934673  [168/200]
Train_loss: 0.178094  Train_acc: 0.935059  [169/200]
Train_loss: 0.182493  Train_acc: 0.935441  [170/200]
Train_loss: 0.181700  Train_acc: 0.935819  [171/200]
Train_loss: 0.179697  Train_acc: 0.936192  [172/200]
Train_loss: 0.179113  Train_acc: 0.936561  [173/200]
Train_loss: 0.180766  Train_acc: 0.936925  [174/200]
Train_loss: 0.183861  Train_acc: 0.937286  [175/200]
Train_loss: 0.177726  Train_acc: 0.937642  [176/200]
Train_loss: 0.180445  Train_acc: 0.937994  [177/200]
Train_loss: 0.179657  Train_acc: 0.938343  [178/200]
Train_loss: 0.181578  Train_acc: 0.938687  [179/200]
Train_loss: 0.177624  Train_acc: 0.939028  [180/200]
Train_loss: 0.180531  Train_acc: 0.939365  [181/200]
Train_loss: 0.177156  Train_acc: 0.939698  [182/200]
Train_loss: 0.181833  Train_acc: 0.940027  [183/200]
Train_loss: 0.179923  Train_acc: 0.940353  [184/200]
Train_loss: 0.177726  Train_acc: 0.940676  [185/200]
Train_loss: 0.177768  Train_acc: 0.940995  [186/200]
Train_loss: 0.179279  Train_acc: 0.941310  [187/200]
Train_loss: 0.178772  Train_acc: 0.941622  [188/200]
Train_loss: 0.178960  Train_acc: 0.941931  [189/200]
Train_loss: 0.182037  Train_acc: 0.942237  [190/200]
Train_loss: 0.183342  Train_acc: 0.942539  [191/200]
Train_loss: 0.178586  Train_acc: 0.942839  [192/200]
Train_loss: 0.179719  Train_acc: 0.943135  [193/200]
Train_loss: 0.182537  Train_acc: 0.943428  [194/200]
Train_loss: 0.178245  Train_acc: 0.943718  [195/200]
Train_loss: 0.177803  Train_acc: 0.944005  [196/200]
Train_loss: 0.180684  Train_acc: 0.944289  [197/200]
Train_loss: 0.181011  Train_acc: 0.944571  [198/200]
Train_loss: 0.189021  Train_acc: 0.944849  [199/200]
Train_loss: 0.181873  Train_acc: 0.945125  [200/200]
Test_acc: 0.725000

End:20220123_23_45_41
Implementation time:14m 32s

